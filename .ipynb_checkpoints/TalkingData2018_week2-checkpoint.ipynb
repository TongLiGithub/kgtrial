{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TalkingData AdTracking Fraud Detection Challenge\n",
    "\n",
    "Can you detect fraudulent click traffic for mobile app ads?\n",
    "\n",
    "## Prerequisites\n",
    "Please make sure the following Python distributions and packages were installed.\n",
    "\n",
    "* [Anaconda](https://anaconda.org)\n",
    "* [XGBoost](https://github.com/dmlc/xgboost)\n",
    "* [LightGBM](https://github.com/Microsoft/LightGBM) - not needed by week 1\n",
    "* [Keras](https://keras.io) - not needed by week 1\n",
    "* [Tensorflow](https://www.tensorflow.org) - not needed by week 1\n",
    "* [Bayesian Optimization](https://github.com/fmfn/BayesianOptimization) - not needed by week 1\n",
    "* [seaborn](https://seaborn.pydata.org)\n",
    "* [bokeh](http://bokeh.pydata.org)\n",
    "\n",
    "You'll also need to create the following sub-folders in your working folder:\n",
    "\n",
    "* input\n",
    "   \n",
    "   To store all the data files downloaded from Kaggle\n",
    "   \n",
    "   \n",
    "* output\n",
    "    \n",
    "    To store submission files\n",
    "   \n",
    "   \n",
    "* python\n",
    "    \n",
    "    To store python scripts and ipython notebooks including this one. Make sure you copied each week's notebook into this folder.\n",
    "    \n",
    "Please note that Kaggle recently released its official API which can be found from https://github.com/Kaggle/kaggle-api\n",
    "\n",
    "With the API you can programmably download files, make submissions and browse yor scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tong\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import lightgbm as lgb\n",
    "from bayes_opt import BayesianOptimization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "from sklearn import preprocessing, pipeline, metrics, model_selection\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "from sklearn.linear_model import LogisticRegression,RidgeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import gc\n",
    "\n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data initialization\n",
    "\n",
    "The first major challenge of this competition is the size of data. With a typical configuration of 16GB RAM it's unlikely that we would be able to performan any serious data manipulations or feature engineerings. Fortunately, there are a few schemes that can help ease the pain. Thanks to Kaggler [yulia](https://www.kaggle.com/yuliagm) who kindly shared a great notebook [How to Work with BIG Datasets on 16G RAM (+Dask)\n",
    "](https://www.kaggle.com/yuliagm/how-to-work-with-big-datasets-on-16g-ram-dask) that explains usefull tips in details.\n",
    "\n",
    "TIP 1 - Deleting unused variables and gc.collect()\n",
    "TIP 2 - Presetting the datatypes\n",
    "TIP 3 - Importing selected rows of the a file (including generating your own subsamples)\n",
    "TIP 4 - Importing in batches and processing each individually\n",
    "TIP 5 - Importing just selected columns\n",
    "TIP 6 - Creative data processing\n",
    "TIP 7 - Using Dask\n",
    "\n",
    "We don't necessarily need to use all of them except TIP 1, 2 and 5. It's up to you to decide if you'd like to use the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Construct a list of rows we want to skip randomly\n",
    "\n",
    "#skiplines = np.random.choice(np.arange(1, 184903890), \n",
    "                             #size=184903890-1-40000000, replace=False)\n",
    "\n",
    "# Predefine data types for each dolumn\n",
    "column_types = {'click_id': 'uint32',\n",
    "                 'app': 'uint16',\n",
    "                 'channel': 'uint16',\n",
    "                 'device': 'uint16',\n",
    "                 'ip': 'uint32',\n",
    "                 'is_attributed': 'uint8',\n",
    "                 'os': 'uint16'}\n",
    "\n",
    "train_data = pd.read_csv('../input/train.csv/train.csv'\n",
    "                         , dtype=column_types\n",
    "                         , usecols=['ip','app','device','os', 'channel', 'click_time', 'is_attributed']\n",
    "                         # We also want to process datatime columns seperately\n",
    "                         , skiprows=range(1,184903890-40000000)\n",
    "                         , nrows=40000000\n",
    "                         , parse_dates = ['click_time']\n",
    "                         , infer_datetime_format=True )\n",
    "\n",
    "test_data = pd.read_csv('../input/test.csv/test.csv'\n",
    "                        , dtype=column_types\n",
    "                        , usecols=['ip','app','device','os', 'channel', 'click_time', 'click_id']\n",
    "                        , parse_dates = ['click_time']\n",
    "                        , infer_datetime_format=True )\n",
    "train_size = train_data.shape[0]\n",
    "\n",
    "print ('train size: ', train_data.shape, 'test size: ', test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The codes above load 40M samples randomly from the training data. However, noticed that the training data was somehow sorted by click time wouldn't it make more sense to load samples from the tail of the file? You substitute the file load statement with following:\n",
    "\n",
    "```python\n",
    "train_data = pd.read_csv('../input/mnt/ssd/kaggle-talkingdata2/competition_files/train.csv'\n",
    "                         , dtype=column_types\n",
    "                         , usecols=['ip','app','device','os', 'channel', 'click_time', 'is_attributed']\n",
    "                         # We also want to process datatime columns seperately\n",
    "                         , skiprows=range(1,184903890-40000000), \n",
    "                         , nrows=40000000\n",
    "                         , parse_dates = ['click_time']\n",
    "                         , infer_datetime_format=True )\n",
    "\n",
    "```\n",
    "\n",
    "If you are interested in automating the process to construct the data types dictionary below is an example for your reference.\n",
    "\n",
    "\n",
    "```python\n",
    "for dtype in ['float','int','category']:\n",
    "    selected_dtype = test_data.select_dtypes(include=[dtype])\n",
    "    mean_usage_b = selected_dtype.memory_usage(deep=True).mean()\n",
    "    mean_usage_mb = mean_usage_b / 1024 ** 2\n",
    "    print(\"Average memory usage for {} columns: {:03.2f} MB\".format(dtype,mean_usage_mb))\n",
    "    \n",
    "```    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge training and testing data\n",
    "This is a trick I used extensively for Kaggle. The idea is to combine train and test dataset so we don't have to perform the same transformations on both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_data=pd.concat([train_data,test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del train_data, test_data\n",
    "full_data['click_id'] = full_data['click_id'].fillna(0).astype('uint32')\n",
    "full_data['is_attributed'] = full_data['is_attributed'].fillna(0).astype('uint8')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_vars = []\n",
    "\n",
    "num_vars = ['ip','app','device','os', 'channel']\n",
    "\n",
    "id_var = 'click_id'\n",
    "target_var = 'is_attributed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing value imputation\n",
    "\n",
    "Missing values are not observed in this competition so we don't need to do anything about it. However, XGBoost and LightGBM have a unique way of dealing with missing values which tend to be effective most of the time. By default, the assume Nan as missing value and would try to create splitting point for those samples with Nan. I would be interesting to try the followings and see which works better:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Categorical features - label encoding\n",
    "\n",
    "Label encoding is not really necessary for this competition as all categorical features have already been digitalized. I'm simply including this just for your reference.\n",
    "\n",
    "```python\n",
    "LBL = preprocessing.LabelEncoder()\n",
    "\n",
    "LE_vars=[]\n",
    "LE_map=dict()\n",
    "for cat_var in cat_vars:\n",
    "    print (\"Label Encoding %s\" % (cat_var))\n",
    "    LE_var=cat_var+'_le'\n",
    "    full_data[LE_var]=LBL.fit_transform(full_data[cat_var].fillna(-1))\n",
    "    LE_vars.append(LE_var)\n",
    "    LE_map[cat_var]=LBL.classes_\n",
    "    \n",
    "print (\"Label-encoded feaures: %s\" % (LE_vars))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical features - one hot encoding¶\n",
    "\n",
    "Performing One-hot encoding for this competition is unlikely going to work due to the size of data. However, if we want to use other algorithms such as MLP we'll still have to do it. \n",
    "\n",
    "Another tip for OHE is that you don't want to concatenate the converted OHE features with the original dataframe(full_data) becuase it would exponentially enlarge the size of the dataframe. In fact, it's recommended to use scipy.sparse.hstack to concatenate the data which you will see in the following sections.\n",
    "\n",
    "\n",
    "```python\n",
    "OHE = preprocessing.OneHotEncoder(sparse=True)\n",
    "start=time.time()\n",
    "OHE.fit(full_data[LE_vars])\n",
    "OHE_sparse=OHE.transform(full_data[LE_vars])\n",
    "                                   \n",
    "print ('One-hot-encoding finished in %f seconds' % (time.time()-start))\n",
    "\n",
    "\n",
    "OHE_vars = [var[:-3] + '_' + str(level).replace(' ','_')\\\n",
    "                for var in cat_vars for level in LE_map[var] ]\n",
    "\n",
    "print (\"OHE_sparse size :\" ,OHE_sparse.shape)\n",
    "print (\"One-hot encoded catgorical feature samples : %s\" % (OHE_vars[:100]))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numeric features\n",
    "\n",
    "For the first two two weeks we will be using XGBoost/LightGBM which typically don't require pre-processing for numeric features so we will skip this part until week 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datetime features\n",
    "\n",
    "Exact datetime elements, e.g hour, day and day of week. Feel free to try extracting others such as minute, day of year and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_data['hour'] = pd.to_datetime(full_data.click_time).dt.hour.astype('uint8')\n",
    "full_data['day'] = pd.to_datetime(full_data.click_time).dt.day.astype('uint8')\n",
    "full_data['dow']  = pd.to_datetime(full_data.click_time).dt.dayofweek.astype('uint8')\n",
    "\n",
    "dt_vars = ['hour','day', 'dow']\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp = full_data[['ip','day','hour','channel']].groupby(by=['ip','day','hour'])[['channel']].count().reset_index().rename(index=str, columns={'channel': 'qty'})\n",
    "tmp['qty'] = tmp['qty'].astype('uint16')\n",
    "full_data = full_data.merge(tmp, on=['ip','day','hour'], how='left')\n",
    "del tmp\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "tmp = full_data[['ip', 'app', 'channel']].groupby(by=['ip', 'app'])[['channel']].count().reset_index().rename(index=str, columns={'channel': 'ip_app_count'})\n",
    "tmp['ip_app_count'] = tmp['ip_app_count'].astype('uint16')\n",
    "full_data = full_data.merge(tmp, on=['ip','app'], how='left')\n",
    "del tmp\n",
    "gc.collect()\n",
    "\n",
    "tmp = full_data[['ip','app', 'os', 'channel']].groupby(by=['ip', 'app', 'os'])[['channel']].count().reset_index().rename(index=str, columns={'channel': 'ip_app_os_count'})\n",
    "tmp['ip_app_os_count'] = tmp['ip_app_os_count'].astype('uint16')\n",
    "full_data = full_data.merge(tmp, on=['ip','app', 'os'], how='left')\n",
    "del tmp\n",
    "gc.collect()\n",
    "\n",
    "interact_vars = ['qty', 'ip_app_count', 'ip_app_os_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_vars = num_vars + dt_vars + interact_vars\n",
    "print ('Training variables are: ', full_vars)\n",
    "train_x = full_data[full_vars][:train_size].values\n",
    "train_y = full_data[target_var][:train_size].values\n",
    "test_x = full_data[full_vars][train_size:].values\n",
    "ids = full_data[id_var][train_size:].values\n",
    "\n",
    "print ('train data size:', train_x.shape, 'test data size:', test_x.shape)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model tuning\n",
    "\n",
    "Cross validation or holdout validation?\n",
    "\n",
    "It really depends. Typicall for time series CV doesn' work very well as in which we could be using \"future\" data to predict \"historical\" data. However, emperically it could work well based on many feedbacks from Kaggle and other communities so it's up to you to decide whether to trust it or not. In practice many DS would choose using a holdout dataset that has the same period of time as in the test set but how to choose the \"right\" size and splitting is really challenging and therefore is the key to build a good ML model for time-series problems. \n",
    "\n",
    "\n",
    "## LightGBM tuning\n",
    "### Manual tuning (greedy search)\n",
    "\n",
    "* Tune one parameter at a time.\n",
    "* Find the best value for a parameter then move to the next one.\n",
    "* Repeat the process for all the parameters we want to tune.\n",
    "* Tuning with a larger learning rate 0.1.\n",
    "* scale_pos_weight needs to be tuned for imbalanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a parameter space\n",
    "metric = 'auc'\n",
    "default_lgb_params = {}\n",
    "default_lgb_params['objective'] = 'binary'\n",
    "default_lgb_params['metric'] = metric\n",
    "default_lgb_params['learning_rate'] = 0.1\n",
    "default_lgb_params['max_bin'] = 255\n",
    "default_lgb_params['nthread'] = 8\n",
    "default_lgb_params['seed'] = 1234\n",
    "\n",
    "params_lgb_space = {}\n",
    "params_lgb_space['scale_pos_weight'] = [1, 50, 99, 500, 1000]\n",
    "params_lgb_space['num_leaves'] = [3, 7, 15, 31, 63, 127, 255, 511, 1023]\n",
    "params_lgb_space['min_gain_to_split'] = [0, 0.1, 0.3, 1, 1.5, 2, 3]\n",
    "params_lgb_space['feature_fraction'] = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "params_lgb_space['bagging_freq'] = [0, 1, 3, 5]\n",
    "params_lgb_space['bagging_fraction'] = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "params_lgb_space['min_sum_hessian_in_leaf'] = [1, 5, 10, 30, 100]\n",
    "params_lgb_space['lambda_l2'] = [0, 0.01, 0.1, 1, 10, 100]\n",
    "params_lgb_space['lambda_l1'] = [0, 0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "greater_is_better = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_lgb_params = default_lgb_params\n",
    "lgtrain = lgb.Dataset(train_x, train_y)\n",
    "for p in params_lgb_space:\n",
    "    print (\"Tuning parameter %s in %s\" % (p, params_lgb_space[p]))\n",
    "\n",
    "    params = best_lgb_params\n",
    "    scores = []    \n",
    "    for v in params_lgb_space[p]:\n",
    "        print ('    %s: %s' % (p, v))\n",
    "        params[p] = v\n",
    "        cv_results = lgb.cv(params, \n",
    "                            lgtrain,\n",
    "                            num_boost_round=1000000,\n",
    "                            early_stopping_rounds=30,\n",
    "                            nfold=4,\n",
    "                            feval=None,\n",
    "                            stratified=True,\n",
    "                            shuffle=True,\n",
    "                            verbose_eval=False)    \n",
    "\n",
    "        cv_results = pd.DataFrame(cv_results)\n",
    "        best_iteration = len(cv_results)        \n",
    "        if greater_is_better:\n",
    "            best_score = cv_results[metric+'-mean'].max()\n",
    "        else:\n",
    "            best_score = cv_results[metric+'-mean'].min()\n",
    "        print (', best_score: %f, best_iteration: %d' % (best_score, best_iteration))\n",
    "        scores.append([v, best_score])\n",
    "    # best param value in the space\n",
    "    best_param_value = sorted(scores, key=lambda x:x[1],reverse=greater_is_better)[0][0]\n",
    "    best_param_score = sorted(scores, key=lambda x:x[1],reverse=greater_is_better)[0][1]\n",
    "    best_lgb_params[p] = best_param_value\n",
    "    print (\"Best %s is %s with a score of %f\" %(p, best_param_value, best_param_score))\n",
    "\n",
    "print ('Best manually tuned parameters:', best_lgb_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automated tuning\n",
    "\n",
    "We will be using a package BayesianOptimization for automated tuning. Results from manual tuning can be used to further narrow the space that needs to be searched from for better performance. \n",
    "\n",
    "Visit https://github.com/fmfn/BayesianOptimization for more details about Bayesian Optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lgb_evaluate(\n",
    "                 num_leaves,\n",
    "                 min_sum_hessian_in_leaf,\n",
    "                 min_gain_to_split,\n",
    "                 feature_fraction,\n",
    "                 bagging_fraction,\n",
    "                 bagging_freq,\n",
    "                 lambda_l1,\n",
    "                 lambda_l2,\n",
    "                 scale_pos_weight\n",
    "                 ):\n",
    "    params = dict()\n",
    "    params['objective'] = 'binary'\n",
    "    params['learning_rate'] = 0.1\n",
    "    params['max_bin'] = 255\n",
    "    params['metric'] = metric\n",
    "    params['num_leaves'] = int(num_leaves)    \n",
    "    params['min_sum_hessian_in_leaf'] = int(min_sum_hessian_in_leaf)\n",
    "    params['min_gain_to_split'] = min_gain_to_split    \n",
    "    params['feature_fraction'] = feature_fraction\n",
    "    params['bagging_fraction'] = bagging_fraction\n",
    "    params['bagging_freq'] = bagging_freq\n",
    "    params['scale_pos_weight'] = scale_pos_weight\n",
    "    params[\"nthread\"] = 8    \n",
    "\n",
    "\n",
    "    cv_results = lgb.cv(params, \n",
    "                        lgtrain,\n",
    "                        num_boost_round=1000000,\n",
    "                        early_stopping_rounds=30,\n",
    "                        nfold=4,\n",
    "                        feval=None,\n",
    "                        stratified=True,\n",
    "                        shuffle=True,\n",
    "                        verbose_eval=False)    \n",
    "\n",
    "    cv_results = pd.DataFrame(cv_results)\n",
    "    best_iteration = len(cv_results)        \n",
    "    if greater_is_better:\n",
    "        best_score = cv_results[metric+'-mean'].max()\n",
    "        print (', best_score: %f, best_iteration: %d' % (best_score, best_iteration))\n",
    "        return best_score\n",
    "    else:\n",
    "        best_score = cv_results[metric+'-mean'].min()\n",
    "        print (', best_score: %f, best_iteration: %d' % (best_score, best_iteration))\n",
    "        return -best_score\n",
    "\n",
    "\n",
    "lgb_BO = BayesianOptimization(lgb_evaluate, \n",
    "                             {\n",
    "                              'num_leaves': (3, 15),\n",
    "                              'min_sum_hessian_in_leaf': (1, 100),\n",
    "                              'min_gain_to_split': (0,2),\n",
    "                              'feature_fraction': (0.3, 1),\n",
    "                              'bagging_fraction': (0.7,1),\n",
    "                              'bagging_freq': (0,5),\n",
    "                              'lambda_l1': (0,1),\n",
    "                              'lambda_l2': (0,1),\n",
    "                              'scale_pos_weight': (99, 1000)\n",
    "                             }\n",
    "                            )\n",
    "\n",
    "lgb_BO.maximize(init_points=5, n_iter=45)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display tuning results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lgb_BO_scores = pd.DataFrame(lgb_BO.res['all']['params'])\n",
    "lgb_BO_scores['score'] = pd.DataFrame(lgb_BO.res['all']['values'])\n",
    "lgb_BO_scores = lgb_BO_scores.sort_values(by='score',ascending=False)\n",
    "lgb_BO_scores.to_csv(\"../python/tuned_lgb_parameters.csv\", index=False)\n",
    "lgb_BO_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.pairplot(lgb_BO_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain the model\n",
    "\n",
    "Now we have the all the parameters optimized except num_boost_round. Recall that the optimized num_boost_round depends on learning_rate(eta) which was set to 0.1, a relative larger number. Let's decrease it to 0.01 so we'll likely need a larger num_boost_round but hopefully would yield a better AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tuned_lgb_params = lgb_BO_scores.iloc[0].to_dict()\n",
    "best_lgb_params = dict()\n",
    "best_lgb_params['objective'] = 'binary'\n",
    "best_lgb_params[\"metric\"] = metric\n",
    "best_lgb_params['learning_rate'] = 0.1 # Smaller learning rate\n",
    "\n",
    "\n",
    "best_lgb_params['scale_pos_weight'] = int(tuned_lgb_params['scale_pos_weight'])   \n",
    "best_lgb_params['num_leaves'] = int(tuned_lgb_params['num_leaves'])    \n",
    "best_lgb_params['min_sum_hessian_in_leaf'] = int(tuned_lgb_params['min_sum_hessian_in_leaf'])\n",
    "best_lgb_params['min_gain_to_split'] = tuned_lgb_params['min_gain_to_split']     \n",
    "best_lgb_params['feature_fraction'] = tuned_lgb_params['feature_fraction']\n",
    "best_lgb_params['bagging_fraction'] = tuned_lgb_params['bagging_fraction']\n",
    "best_lgb_params['bagging_freq'] = tuned_lgb_params['bagging_freq']\n",
    "best_lgb_params['lambda_l1'] = tuned_lgb_params['lambda_l1']\n",
    "best_lgb_params['lambda_l2'] = tuned_lgb_params['lambda_l2']\n",
    "\n",
    "print (best_lgb_params)\n",
    "\n",
    "cv_results = lgb.cv(best_lgb_params, \n",
    "                    lgtrain,\n",
    "                    num_boost_round=1000000,\n",
    "                    early_stopping_rounds=200, # larger earlyer stopping round is needed \n",
    "                    nfold=4,\n",
    "                    feval=None,\n",
    "                    stratified=True,\n",
    "                    shuffle=True,\n",
    "                    verbose_eval=10)    \n",
    "\n",
    "cv_results = pd.DataFrame(cv_results)\n",
    "best_lgb_iteration = len(cv_results)        \n",
    "if greater_is_better:\n",
    "    best_lgb_score = cv_results[metric+'-mean'].max()\n",
    "else:\n",
    "    best_lgb_score = cv_results[metric+'-mean'].min()\n",
    "\n",
    "print (', best_score: %f, best_iteration: %d' % (best_lgb_score, best_lgb_iteration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = lgb.train(best_lgb_params,\n",
    "                  lgb.Dataset(train_x, train_y),\n",
    "                  num_boost_round=best_lgb_iteration\n",
    "                  )\n",
    "\n",
    "preds = model.predict(test_x)\n",
    "\n",
    "sub_df = pd.DataFrame({'click_id': ids, 'is_attributed': preds})\n",
    "sub_df.to_csv(\"../output/lgb_tuned_paras_5M.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Recall that in order to speed up the tuning process we actually used only 5M training samples so the submission above may not yield a better result on LB. Can you try training the model with the same amount of training samples and see how it works? You may want to ** leverage the parameters tuned above** without having to rerun the entire process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost (optional)\n",
    "\n",
    "The following codes are optional as they may take longer time to run. Basically the process is much similar as what's covered above for LightGBM.\n",
    "\n",
    "### Manual tuning (greedy search)\n",
    "\n",
    "* Tune one parameter at a time.\n",
    "* Find the best value for a parameter then move to the next one.\n",
    "* Repeat the process for all the parameters we want to tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "default_xgb_params = {}\n",
    "default_xgb_params[\"objective\"] = \"binary:logistic\"\n",
    "default_xgb_params[\"eta\"] = 0.15\n",
    "default_xgb_params[\"seed\"] = 1234\n",
    "default_xgb_params[\"metric\"] = metric\n",
    "\n",
    "params_xgb_space = {}\n",
    "params_xgb_space['scale_pos_weight'] = [1, 50, 99, 500, 1000, 5000, 10000]\n",
    "params_xgb_space['max_depth'] = [4,5,6,7,8,9,10]\n",
    "params_xgb_space['gamma'] = [0, 0.1, 0.3, 1, 1.5, 2, 3]\n",
    "params_xgb_space['colsample_bytree'] = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "params_xgb_space['subsample'] = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "params_xgb_space['min_child_weight'] = [0, 1, 3, 10, 30, 100]\n",
    "\n",
    "greater_is_better = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_xgb_params = default_xgb_params\n",
    "xgtrain = xgb.DMatrix(train_x, label=train_y)\n",
    "for p in params_xgb_space:\n",
    "    print (\"Tuning parameter %s in %s\" % (p, params_xgb_space[p]))\n",
    "\n",
    "    params = best_xgb_params\n",
    "    scores = []    \n",
    "    for v in params_xgb_space[p]:\n",
    "        print ('    %s: %s' % (p, v), end=\"\")\n",
    "        params[p] = v\n",
    "        cv_results = xgb.cv(params, \n",
    "                            xgtrain,\n",
    "                            nfold=4,   \n",
    "                            num_boost_round=100000,\n",
    "                            early_stopping_rounds=30,\n",
    "                            metrics=metric,\n",
    "                            stratified=True,\n",
    "                            shuffle=True,\n",
    "                            verbose_eval=False)    \n",
    "        best_iteration = len(cv_results)\n",
    "        if greater_is_better:\n",
    "            best_score = cv_results['test-'+metric+'-mean'].max()\n",
    "        else:\n",
    "            best_score = cv_results['test-'+metric+'-mean'].min()\n",
    "        print (', best_score: %f, best_iteration: %d' % (best_score, best_iteration))\n",
    "        scores.append([v, best_score])\n",
    "    # best param value in the space\n",
    "    best_param_value = sorted(scores, key=lambda x:x[1],reverse=greater_is_better)[0][0]\n",
    "    best_param_score = sorted(scores, key=lambda x:x[1],reverse=greater_is_better)[0][1]\n",
    "    best_xgb_params[p] = best_param_value\n",
    "    print (\"Best %s is %s with a score of %f\" %(p, best_param_value, best_param_score))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xgb_evaluate(scale_pos_weight,\n",
    "                 min_child_weight,\n",
    "                 colsample_bytree,\n",
    "                 max_depth,\n",
    "                 subsample,\n",
    "                 gamma):\n",
    "    params = dict()\n",
    "    params['objective'] = 'binary:logistic'\n",
    "    params['eta'] = 0.15\n",
    "    params['scale_pos_weight'] = scale_pos_weight\n",
    "    params['max_depth'] = int(max_depth )   \n",
    "    params['min_child_weight'] = int(min_child_weight)\n",
    "    params['colsample_bytree'] = colsample_bytree\n",
    "    params['subsample'] = subsample\n",
    "    params['gamma'] = gamma\n",
    "    params['seed'] = 1234    \n",
    "    \n",
    "\n",
    "    cv_results = xgb.cv(params, \n",
    "                        xgtrain,\n",
    "                        nfold=4,   \n",
    "                        num_boost_round=100000,\n",
    "                        early_stopping_rounds=30,\n",
    "                        metrics=metric,\n",
    "                        stratified=True,\n",
    "                        shuffle=True,\n",
    "                        verbose_eval=False)    \n",
    "    best_iteration = len(cv_results)\n",
    "    if greater_is_better:\n",
    "        best_score = cv_results['test-'+metric+'-mean'].max()\n",
    "    else:\n",
    "        best_score = -cv_results['test-'+metric+'-mean'].min()\n",
    "    print (', best_score: %f, best_iteration: %d' % (best_score, best_iteration))\n",
    "\n",
    "    return best_score\n",
    "\n",
    "\n",
    "xgb_BO = BayesianOptimization(xgb_evaluate, \n",
    "                             {'scale_pos_weight': (99, 1000),\n",
    "                              'max_depth': (5, 11),\n",
    "                              'min_child_weight': (0, 200),\n",
    "                              'colsample_bytree': (0.2, 1),\n",
    "                              'subsample': (0.7, 1),\n",
    "                              'gamma': (0, 3)\n",
    "                             }\n",
    "                            )\n",
    "\n",
    "xgb_BO.maximize(init_points=5, n_iter=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_BO_scores = pd.DataFrame(xgb_BO.res['all']['params'])\n",
    "xgb_BO_scores['score'] = pd.DataFrame(xgb_BO.res['all']['values'])\n",
    "xgb_BO_scores = xgb_BO_scores.sort_values(by='score',ascending=False)\n",
    "xgb_BO_scores.head()\n",
    "lgb_BO_scores.to_csv(\"../python/tuned_lgb_parameters.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_xgb_params = xgb_BO_scores.iloc[0].to_dict()\n",
    "best_xgb_params['objective'] = 'reg:linear'\n",
    "best_xgb_params['eta'] = 0.05  # Smaller\n",
    "\n",
    "best_xgb_params['scale_pos_weight'] = int(best_xgb_params['scale_pos_weight'])\n",
    "best_xgb_params['max_depth'] = int(best_xgb_params['max_depth'])\n",
    "best_xgb_params['min_child_weight'] = int(best_xgb_params['min_child_weight'])\n",
    "best_xgb_params['subsample'] = best_xgb_params['subsample']\n",
    "best_xgb_params['colsample_bytree'] = best_xgb_params['colsample_bytree']\n",
    "best_xgb_params['gamma'] = best_xgb_params['gamma']\n",
    "best_xgb_params['seed'] = 1234\n",
    "\n",
    "print (best_xgb_params)\n",
    "\n",
    "cv_results = xgb.cv(params, \n",
    "                    xgtrain,\n",
    "                    nfold=4,   \n",
    "                    num_boost_round=100000,\n",
    "                    early_stopping_rounds=30,\n",
    "                    metrics=metric,\n",
    "                    stratified=True,\n",
    "                    shuffle=True,\n",
    "                    verbose_eval=False)    \n",
    "best_xgb_iteration = len(cv_results)\n",
    "if greater_is_better:\n",
    "    best_xgb_score = cv_results['test-'+metric+'-mean'].max()\n",
    "else:\n",
    "    best_xgb_score = -cv_results['test-'+metric+'-mean'].min()\n",
    "print ('best_score: %f, best_iteration: %d' % (best_xgb_score, best_xgb_iteration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = xgb.train(best_xgb_params, \n",
    "                  xgb.DMatrix(train_x, label=train_y), \n",
    "                  num_boost_round=best_xgb_iteration)    \n",
    "preds = model.predict(xgb.DMatrix(test[full_vars].values))\n",
    "\n",
    "sub_df = pd.DataFrame({'click_id': ids, 'is_attributed': preds})\n",
    "sub_df.to_csv(\"../output/xgb_tuned_paras_5M.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "Paramter tuning plays an important role in practical data science. In this we've learnt:\n",
    "\n",
    "1. A framework that can be used to guide parameter turnings\n",
    "2. XGBoost and LightGBM hypter-parameters\n",
    "3. Manual tuning\n",
    "4. Automated tuning\n",
    "\n",
    "\n",
    "# References\n",
    "* [XGBoost Parameters](https://github.com/dmlc/xgboost/blob/master/doc/parameter.md)\n",
    "* [XGBoost official tuning guide](http://xgboost.readthedocs.io/en/latest/how_to/param_tuning.html)\n",
    "* [Tianqi Chen’s guide on XGBoost in Chinese](http://www.52cs.org/?p=429)\n",
    "* [MachineLearningMastery’s XGBoost tutorials](http://machinelearningmastery.com/category/xgboost/)\n",
    "* [Complete Guide to Parameter Tuning in XGBoost](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/)\n",
    "* [What to optimize first? Gamma or Depth? What’s up with min_child_weight?](https://medium.com/data-design/xgboost-hi-im-gamma-what-can-i-do-for-you-and-the-tuning-of-regularization-a42ea17e6ab6)\n",
    "* [LightGBM Parameters](https://github.com/Microsoft/LightGBM/blob/master/docs/Parameters.rst)\n",
    "* [Parameters Tuning](http://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html)\n",
    "* [CatBoost vs. Light GBM vs. XGBoost](https://towardsdatascience.com/catboost-vs-light-gbm-vs-xgboost-5f93620723db)\n",
    "\n",
    "\n",
    "# Assignments\n",
    "* Train LightGBM model with larger amount of training samples with tuned parameters.\n",
    "* Optimize parameters for XGBoost\n",
    "* Did you observe any differences before and after parameter tuning? Any differencew between XGBoost and LightGBM? What is your thought on the differences and what caused the differences?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
